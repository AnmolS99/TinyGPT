# TinyGPT

A tiny GPT (Generative Pretrained Transformer) written from scratch in Python, using Pytorch. Created by following [Andrej Karpathy's tutorial on YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY).

Karpathy's tutorial shows how to create a decoder-only transformer (same structure as OpenAI's GPT) based on the famous [_Attention is All You Need_ paper](https://arxiv.org/abs/1706.03762).

The architecture of transformer is shown below. The right side of the architecture is the decoder part (except for the Multi-Head Attention recieving key and values from encoder part):

<img src="./README_imgs/transformer.png" width="300" alt="Transformer architecture">

## Results âœ…

Karpathy managed to get a validation loss of ~1.48 when training for 5000 iterations on the tiny shakespear dataset. He trained on a A100 GPU, and was therefore able to train a larger transformer with a higher dimensional embedding space.

I trained on an Apple M2 Pro, and therefore had a smaller transformer architecture which I trained for 6000 iterations. On the same dataset, I was able to get a validation loss of ~1.67. As I was training on an Apple silicon chip, I set PyTorch device to "mps" which accelerated training. The training took about 10 minutes. Below is the training run, and also some text generated by the trained model:

<img src="./README_imgs/train_tiny_shakespear.png" width="400" alt="Generated Text when training on Tiny Shakespear">

Out of curiosity I also trained a model on _The Hitchhiker's Guide to the Galaxy_, which is a book I have recently read and enjoyed. The result is shown below:

<img src="./README_imgs/train_hhgttg.png" width="600" alt="Generated Text when training on HHGTTG">

The generated text are nonsensical and have a lot of made-up words, but they still manage to get the structure of the text as well as some words/names right!
